{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Airline_SentimentAnalysis_LIGHTGBM",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26jQeNdn4-67",
        "outputId": "6135d687-cc16-4e4b-b7f9-61ba545359db"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cD60UDO05IgE",
        "outputId": "7dceed32-e347-4eef-904d-128fa394a60b"
      },
      "source": [
        "!pip install emoji\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import re\n",
        "import emoji\n",
        "from nltk.stem import PorterStemmer as ps\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.4.1.tar.gz (185 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▊                              | 10 kB 21.6 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 20 kB 28.2 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 30 kB 18.5 MB/s eta 0:00:01\r\u001b[K     |███████                         | 40 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 61 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 71 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 81 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 92 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 185 kB 5.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.4.1-py3-none-any.whl size=186393 sha256=7819de376378ef98d1c54566cf7689b23abe8287ab308f5afc86c752d153757e\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/68/ac/537456a5331f1405779f2b3c2a578429d2f6d7419e440330d8\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX8MhmXH6FYG"
      },
      "source": [
        "csv_path = \"/content/drive/MyDrive/ENTHIRE/airline_sentiment_analysis.csv\"\n",
        "\n",
        "class airline_data():\n",
        "    def __init__(self,data_dir_path):\n",
        "        self.df = pd.read_csv(data_dir_path)                                        #loading the csv file using pandas\n",
        "        self.df = pd.concat([self.df[\"text\"],self.df[\"airline_sentiment\"]],axis=1) #concatenating the neccesary infromation from the data\n",
        "    \n",
        "    def process_text(self,text):\n",
        "        new_text = text.lower()                                   #making all the characters lower case \n",
        "        new_text = re.sub(r'@\\w+', '', new_text)                  # Remove @s\n",
        "        new_text = re.sub(r'#', '', new_text)                     # Remove hashtags\n",
        "        new_text = re.sub(r':', ' ', emoji.demojize(new_text))    # Turn emojis into words\n",
        "        new_text = re.sub(r'http\\S+', '',new_text)                # Remove URLs\n",
        "        new_text = re.sub(r'\\$\\S+', 'dollar', new_text)           # Change dollar amounts to dollar\n",
        "        new_text = re.sub(r'[^a-z0-9\\s]', '', new_text)           # Remove punctuation\n",
        "        new_text = re.sub(r'[0-9]+', 'number', new_text)          # Change number values to number\n",
        "        new_text = new_text.split(\" \")                            # splits the text into a list of strings after breaking the given text by the specified separator in our case it is \" \".\n",
        "        new_text = list(map(lambda x: ps().stem(x), new_text))    # Stemming the words\n",
        "        new_text = list(map(lambda x: x.strip(), new_text))       # Stripping whitespace from the words\n",
        "        if '' in new_text:\n",
        "            new_text.remove('')\n",
        "        return new_text                                           # returns sentence of words in the form of a list \n",
        "\n",
        "    def preprocess_data(self):\n",
        "      self.Texts = self.df[\"text\"].apply(self.process_text)                                                #cleans all the texts using the process text function\n",
        "      sentiment_ordering = ['negative','positive']\n",
        "      self.labels = self.df[\"airline_sentiment\"].apply(lambda x:sentiment_ordering.index(x))               #converts the positive and negative labels to 1 and 0 respecctively\n",
        "      \n",
        "\n",
        "    def retrieve_vocab_info(self):\n",
        "      self.preprocess_data()\n",
        "      vocabulary = set()\n",
        "      for text in self.Texts:                                                                               #goes through all the words in the data and adds the distinct words to the vocabulary\n",
        "          for word in text:\n",
        "                  vocabulary.add(word)\n",
        "\n",
        "      self.vocab_length = len(vocabulary)   \n",
        "      self.max_seq_length = 0\n",
        "      for text in self.Texts:                                                                               #finds the length of sentence with maximum length out of all the sequences in the Texts.  \n",
        "          if len(text) > self.max_seq_length:\n",
        "              self.max_seq_length = len(text)\n",
        "      return self.vocab_length,self.max_seq_length\n",
        "\n",
        "    def tokenize_words(self):\n",
        "      self.retrieve_vocab_info()                                                                             #retrieves information about the vocabulary length and maximum seqence length  \n",
        "      self.training_sentences,self.testing_sentences,self.y_train,self.y_test = train_test_split(self.Texts,self.labels,train_size=0.7,random_state=10)  #splits the data into training and testing data\n",
        "      tokenizer = Tokenizer(num_words=self.vocab_length,oov_token=\"<OOV>\")  \n",
        "      tokenizer.fit_on_texts(self.training_sentences)                                                        # maps all  distinct words in the training_sentences to numbers \n",
        "      word_index = tokenizer.word_index                                                                      # tokenizer.word_index is dictionary with words as keys and numbers as values formed by fitting the tokenizer on the training sentences\n",
        "      self.training_sequences = tokenizer.texts_to_sequences(self.training_sentences)                        #creates sequences of tokens representing each sentence\n",
        "      self.X_train = pad_sequences(self.training_sequences, maxlen=self.max_seq_length, padding='post')      #pads the sequence with zeros at the end to regularise the length of all the sequences to the maximum sequence length\n",
        "\n",
        "      self.testing_sequences = tokenizer.texts_to_sequences(self.testing_sentences)\n",
        "      self.X_test = pad_sequences(self.testing_sequences, maxlen=self.max_seq_length, padding='post')\n",
        "      \n",
        "      pickle.dump(tokenizer,open(\"/content/drive/MyDrive/ENTHIRE/GRU_Model/tokenizer_file3.pkl\",\"wb\"))       #saving the tokenizer using pickle so it can be used while deploying the model using fastAPI\n",
        "\n",
        "      return self.X_train,self.X_test,self.y_train,self.y_test \n",
        "\n",
        "# DATA LOADING \n",
        "data = airline_data(csv_path)  \n",
        "vocab_length,max_seq_length = data.retrieve_vocab_info()\n",
        "X_train,X_test,y_train,y_test = data.tokenize_words() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkddCTCV7H9g"
      },
      "source": [
        "\n",
        "import lightgbm as lgb\n",
        "import joblib\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "from sklearn import preprocessing\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfoSR0Z-6OP8",
        "outputId": "f0a0eba8-e670-4ba0-be10-63202ca7eb5f"
      },
      "source": [
        "# %%time\n",
        "RESULTS_FOLDER=\"/content/drive/MyDrive/ENTHIRE/\"\n",
        "\n",
        "#representing the dataset in the form that lightGBM Dataset class\n",
        "d_train=lgb.Dataset(X_train, label=y_train)\n",
        "\n",
        "#Specifying the parameter\n",
        "params={}\n",
        "params['learning_rate']=0.03\n",
        "params['boosting_type']='gbdt' #GradientBoostingDecisionTree\n",
        "params['objective']='binary' #Binary target feature\n",
        "params['metric']='binary_logloss' #metric for binary classification\n",
        "params['max_depth']=10\n",
        "\n",
        "#training the model on training_data for 100 epochs\n",
        "model = lgb.train(params,d_train,100)\n",
        "\n",
        "\n",
        "# Save the model\n",
        "joblib.dump(model, os.path.join(RESULTS_FOLDER, 'LGBM_Model_2.pkl'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/ENTHIRE/LGBM_Model_2.pkl']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh_y907T7E86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d860f43b-1f22-4646-c7b5-475d2c82832b"
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "y_pred = y_pred.round(0) #if value less than or equal 0.5 it outputs 0 else it outputs 1.\n",
        "print(\"ACCURACY: {}\".format(accuracy_score(y_pred,y_test))) \n",
        "print(\"F1_SCORE: {}\".format(f1_score(y_pred,y_test))) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ACCURACY: 0.8362691308114352\n",
            "F1_SCORE: 0.40752351097178685\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}